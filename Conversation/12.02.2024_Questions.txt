Progress Report:
- API-Pipelines und Evaluation für Sentiment Analysis & Schema Matching aufgesetzt
- Für Sentiment Analysis erste F1 Values für GPT-3.5-Turbo-Instruct / Zero-Shot:
                  precision    recall  f1-score   support

    negative       0.56      0.65      0.60       329
     neutral       0.57      0.46      0.51       607
    positive       0.56      0.65      0.60       400

    accuracy                           0.57      1336
   macro avg       0.56      0.59      0.57      1336
weighted avg       0.57      0.57      0.56      1336



1. Welches Datenset für die GPT API Abfragen - hier gibt es ja nichts zu trainieren. Brauche ich trainings / validation set dann überhaupt? Abfragen mache ich nur mit dem Testset?

2. Immer Maximal vorgegebene Größe der Testsets nutzen oder kann ich auch einheitliche Größe verwenden? Bei Sentiment Analyse über 1300 abfragen - hohe kosten, vor allem bei GPT-4 - ggf. die größe der Sets einschränken? Wollte das hier kurz abklären, bevor ich es auf GPT-4 laufen lasse und es unnötig hohe Kosten verursacht

3. Bzgl. Car Price Prediction: F1 geht ja hier nicht, also würde ich MSE, MAE, R-Squared hier als zielmetriken errechnen?

4. Aufteilen des Car Price Prediction Datenset - mein vorgeschlagenes Vorgehen:
    - Random Sampling, da alle Einträge relativ einheitlich
    - Aufteilung: 10% Test, 10% Validation, 80% Trainingsset

5. Vorgehen bei Few-Shot? Einmal fixe Examples (random-sampling) und einmal k-nearest?


6. Ein paar kurze Fragen zu Schema Matching:
a) Bzgl. der Schema Matchings bin ich mir noch unsicher wie genau ich hier auf den F1 Wert kommen. 
In test_correspondences.csv 

b) Du hattest ja angemerkt, dass in den Datensets keine Fehler sein dürfen. Wie verhält sich das ganze mit Null values in dem Schema Matching Tables? Soll ich spezifisch Einträge mit zu vielen Null values rausnehmen oder ist das als Noise so gewollt?

Github Link: https://github.com/timgutberlet/Error-Analysis-with-LLMs
